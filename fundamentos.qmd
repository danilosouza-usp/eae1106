# Fundamentos de computação {#sec-fundamentos}

In summary, this book has no content whatsoever.


## O que é um computador?

Um computador realiza duas coisas e apenas duas coisas: faz operações e guarda de forma eficiente o resultado dessas operações. No entanto, ao fazer essas duas coisas de forma extremamente eficiente essa máquina chamada computador nos permite receber, armazenar, processar e transmitir informações. Em essência, o computador nos serve ao propósito de **resolução de problemas**. Isso significa que, através de uma linguagem formal (especificamente operações de computação), podemos formular problemas e **programar** meios de resolver de forma automatizada tais problemas. No fim das contas, programar é o ato de criar programas, isto é, estabelecer uma sequência de instruções em linguagem formal que especifica como executar uma determinada operação no computador. 

Mas antes de avançar na ideia de **programa** e **algoritmos**, quais são as características de um computador como o conhecemos?

### Sistema computacional

Um sistema computacional é resultado da integração de componentes atuando como uma entidade, com o propósito de processar dados, isto é, realizar algum tipo de operação lógica envolvendo os dados, de modo a produzir diferentes níveis de informações.
Componentes: 

* **Hardware**: componente física de um sistema de computação, isto é, todos os equipamentos utilizados pelo usuário nas ações de entrada, processamento, armazenamento e saída de dados.
* **Software**: componente lógica de um sistema de computação, isto é, séries de instruções que fazem o computador funcionar (programas de computador)

E por fim o componente humano do sistema, as pessoas, que utilizam o computador basicamente como ferramenta para atingir determinado fim, para **resolver problemas**.

### Arquitetura de um computador

Os computadores, como os conhecemos hoje, são estruturados em cima da lógica proposta inicialmente por John Von Neumann, matemático húngaro que viveu durante a primeira metade do século XX e que contribuiu imensamente em várias dimensões das ciências. Embora tenhamos hoje uma complexidade maior nos elementos que compõe a arquitetura de um computador, a estrutura básica por trás dos computadores mais modernos continua sendo aquela proposta por Von Neumann. Essa arquitetura pode ser representado pelo diagrama abaixo:

![Diagrama que representa o funcionamento da arquitetura Von-Neumann](images/arquitetura_VonNeumann.jpg){#fig-arquitetura width=700px max-width=100% fig-align="center"}

Essencialmente, o hadware que compõe as partes da arquitetura de um computador podem ser dividas em 3 partes:

* **Dispositivos de entrada e saída**: é através deles que o usuário dialoga com a máquina. Através do teclado e mouse, por exemplo, o usuário fornece informações ao computador a partir das quais processos serão realizados e seus resultados serão percebidos pelos dispositivos de saída (monitor e impressora, por exemplo).

* **Unidade central de processamento (CPU)**: é o cérebro do sistema computacional, é nele que serão executados cálculos e instruções lógicas dos usuários. 

* **Memória**: aqui são armazenados os resultados dos cálculos, dados e instruções.

    
### Capacidade de armazenamento

* Kilobyte: 1024 (2^10) bytes.
    + Capacidade de memória dos computadores pessoais muito antigos.
    
   <br>
   
* Megabyte: aproximadamente, um milhão (2^20) de bytes.
    + Memória de computadores pessoais antigos.
    
   <br>
     
     
* Gigabyte: aproximadamente, um bilhão (2^30) de bytes.
    + Dispositivos de armazenamento (discos rígidos), especialmente SSD
    
   <br>
    
* Terabyte: aproximadamente, um trilhão (2^40) de bytes.
    + Dispositivos de armazenamento para sistemas muito grandes e servidores.
    
   <br>
     
### Como computadores armazenam números

Para entender a diferença entre computação de precisão simples e de precisão dupla, é importante entender o papel da precisão na ciência da computação. Imagine realizar um cálculo usando um número irracional (como $\pi$) e incluindo apenas dois dígitos à direita do ponto decimal (3.14). Você obteria um resultado mais preciso se fizesse o cálculo incluindo dez dígitos à direita do ponto decimal (3,1415926535).

Para computadores, esse nível de precisão é chamado de precisão e é medido em dígitos binários (bits) em vez de casas decimais. Quanto mais bits usados, maior a precisão. A representação de grandes números em binários de computador requer um padrão para garantir que não haja grandes discrepâncias nos cálculos. Assim, o Instituto de Engenheiros Elétricos e Eletrônicos (IEEE) desenvolveu o Padrão IEEE para Aritmética de Ponto Flutuante (IEEE 754).

Em geral, um número será representado de forma aproximada com um número fixo de números significantes e escalonado usando um expoente em uma base (usualmente dois, ou 10 ou 16). Esses números possuem a seguinte forma:

$$ S \times B^e $$

Em que $S$ é o significando, $B$ é um número inteiro maior ou igual a dois, e $e$ um expoente, um número inteiro. Por exemplo:
$1.2345=12345 \times 10^{-4}$. O termo Ponto Flutuante se refere ao fato que o ponto decimal (também chamado de ponto binário) pode flutuar; ou seja, pode ser colocado em qualquer lugar em relação aos dígitos significantes do número. Esta posição é indicada pelo expoente, e a representação em ponto flutuante de um número pode ser pensada como uma forma de notação científica. 

A precisão nessa representação de ponto flutuante é dada pela quantidade de bits dedicado ao armazenamento do significando e do expoente -- quanto maior o significando, mais precisa é a representação (menor é o espaço entre dois números), e quanto maior o expoente, mais alto é o maior valor que pode ser representado.

Existem três componentes do IEEE 754:

* O sinal - 0 representa um número positivo; 1 representa um número negativo.
* O expoente - O expoente é usado para representar expoentes positivos e negativos. 
* O significando - Também conhecida como mantissa, a mantissa representa os bits de precisão do número.

Usando esses componentes, o IEEE 754 representa números de ponto flutuante de duas maneiras: formato de precisão simples e formato de precisão dupla. Embora ainda existam várias maneiras de representar números de ponto flutuante, o IEEE 754 é o mais comum porque geralmente é a representação mais eficiente de valores numéricos.

### Formato de ponto flutuante de precisão simples

O formato de ponto flutuante de precisão simples usa 32 bits (um bit pro sinal, 8 pro expoente e 24 pro significando) de memória do computador e pode representar uma ampla gama de valores numéricos. Muitas vezes referido como FP32, esse formato é melhor usado para cálculos que não sofrem um pouco de aproximação.

### Formato de ponto flutuante de precisão dupla

O formato de ponto flutuante de precisão dupla, por outro lado, ocupa 64 bits (um bit pro sinal, 11 pro expoente e 52 pro significando) da memória do computador e é muito mais preciso do que o formato de precisão simples. Esse formato é frequentemente chamado de FP64 e usado para representar valores que exigem um intervalo maior ou um cálculo mais preciso.

Embora a precisão dupla permita mais precisão, ela também requer mais recursos computacionais, armazenamento de memória e transferência de dados. O custo de usar esse formato nem sempre faz sentido para todos os cálculos.

<br>

### Mas qual a importância disso tudo?

Saber como os computadores armazenam números, ou qual a importância da memória no processo de fornecimento de parâmetros de entrada em busca de uma saída específica, nos ajuda enormemente a entender porque, por exemplo, um sistema 32 bits só é capaz de nos fornecer 4GB de memória RAM para armazenamento de cálculos e operações ou mesmo porque podemos ter dificuldade ao tentar abrir no nosso computador local um arquivo com alguns milhões de linhas. 

É importante ter o conhecimento mínimo de como funciona um computador e como os programas que iremos utilizar interagem com as partes da arquitetura do computador para entendermos o que está ocorrendo atrás das cortinas. Dessa forma, podemos nos concentrar exclusivamente no que nos interessa, isto é, utilizar o computador como ferramenta para **resolver problemas** e não como um fim em si (afinal estamos em um curso de economia e não de ciência da computação). Para tal precisamos de alguma linguagem lógica/formal que nos permita enviar as intruções para o computador e obter as respostas desejadas. Como já deve ter ficado claro, nesse curso trabalharemos com a linguagem **Python**.



## Linguagens de programação 

As linguagens naturais são os idiomas que as pessoas falam, como inglês, espanhol e francês. Elas não foram criadas pelas pessoas (embora as pessoas tentem impor certa ordem a elas); desenvolveram-se naturalmente. As linguagens formais são linguagens criadas pelas pessoas para aplicações específicas. Por exemplo, a notação que os matemáticos usam é uma linguagem formal especialmente boa para denotar relações entre números e símbolos. Os químicos usam uma linguagem formal para representar a estrutura química de moléculas. E o mais importante:

**As linguagens de programação são idiomas formais criados para expressar operações de computação.**

As linguagens formais geralmente têm regras de sintaxe estritas que governam a estrutura de declarações. Por exemplo, na matemática a declaração $3 + 3 = $6 tem uma sintaxe correta, mas não $3 + = 3\$6$. Na química, H2O é uma fórmula sintaticamente correta, mas 2Zz não é. As regras de sintaxe vêm em duas categorias relativas a símbolos e estrutura. Os símbolos são os elementos básicos da linguagem, como palavras, números e elementos químicos. Um dos problemas com $3 + = 3\$6$ é que o '\$' não é um símbolo legítimo na matemática (pelo menos até onde eu sei). De forma similar, 2Zz não é legítimo porque não há nenhum elemento com a abreviatura Zz.

O segundo tipo de regra de sintaxe refere-se ao modo no qual os símbolos são combinados. A equação 3 + = 3 não é legítima porque, embora + e = sejam símbolos legítimos, não se pode ter um na sequência do outro. De forma similar, em uma fórmula química o subscrito vem depois do nome de elemento, não antes.

Esta é um@ frase bem estruturada em portuguê\$, mas com s\*mbolos inválidos.

Ao ler uma frase em português ou uma declaração em uma linguagem formal, é preciso compreender a estrutura (embora em uma linguagem natural você faça isto de forma subconsciente). Este processo é chamado de análise. Embora as linguagens formais e naturais tenham muitas características em comum – símbolos, estrutura e sintaxe – há algumas diferenças:

1. **Ambiguidade**: as linguagens naturais são cheias de ambiguidade e as pessoas lidam com isso usando pistas contextuais e outras informações. As linguagens formais são criadas para ser quase ou completamente inequívocas, ou seja, qualquer afirmação tem exatamente um significado, independentemente do contexto

2. **Redundância**: para compensar a ambiguidade e reduzir equívocos, as linguagens naturais usam muita redundância. Por causa disso, muitas vezes são verborrágicas. As linguagens formais são menos redundantes e mais concisas.

3. **Literalidade**: as linguagens naturais são cheias de expressões e metáforas. Se eu digo "caiu a ficha", provavelmente não há ficha nenhuma na história, nem nada que tenha caído (esta é uma expressão para dizer que alguém entendeu algo depois de certo período de confusão). As linguagens formais têm significados exatamente iguais ao que expressam.

Como todos nós crescemos falando linguagens naturais, às vezes é difícil se ajustar a linguagens formais. As linguagens formais são mais densas que as naturais, então exigem mais tempo para a leitura. Além disso, a estrutura é importante, então nem sempre é melhor ler de cima para baixo e da esquerda para a direita. Em vez disso, aprenda a analisar o programa primeiro, identificando os símbolos e interpretando a estrutura. E os detalhes fazem diferença. Pequenos erros em ortografia e pontuação, que podem não importar tanto nas linguagens naturais, podem fazer uma grande diferença em uma língua formal.


### Alto nível X Baixo nível

Na ciência da computação, uma **linguagem de programação de alto nível** é uma linguagem de programação com forte abstração dos detalhes do computador. São linguagens mais próximas das linguagens humanas e mais distantes das linguagens de máquina. Em contraste com as linguagens de programação de baixo nível, ela pode usar elementos de linguagem natural, ser mais fácil de usar, ou pode automatizar (ou até ocultar totalmente) áreas significativas de sistemas de computação (por exemplo, gerenciamento de memória), tornando o processo de desenvolvimento de um programa mais simples e eficiente. Exemplos de linguagens de alto nível: Python, JavaScript.

Por outro lado, uma **linguagem de baixo nível** é uma linguagem de programação que fornece pouca ou nenhuma abstração de conceitos de programação e está muito próxima de escrever instruções de máquina reais. A palavra "baixo" refere-se à pequena ou inexistente quantidade de abstração entre a linguagem e a linguagem de máquina; por causa disso, as linguagens de baixo nível são às vezes descritas como "próximas do hardware". Programas escritos em linguagens de baixo nível tendem a ser relativamente não portáteis e dependentes do computador para o qual foram escritas. Exemplo de linguagem de baixo nível: Assembly


### Linguagem Compilada X Interpretada

As **linguagens compiladas** são convertidas diretamente em código de máquina que o processador pode executar. Como resultado, elas tendem a ser mais rápidos e eficientes de executar do que linguagens interpretadas. Eles também dão ao desenvolvedor mais controle sobre os aspectos de hardware, como gerenciamento de memória e uso da CPU. As linguagens compiladas precisam de uma etapa de "construção" – elas precisam ser compiladas manualmente primeiro. Você precisa "reconstruir" o programa toda vez que precisar fazer uma alteração. Exemplos de linguagens compiladas puras: C, C++, Go.

Uma **linguagem interpretada** é uma linguagem de programação que geralmente é interpretada, sem compilar um programa em instruções de máquina. É aquela em que as instruções não são executadas diretamente pela máquina de destino, mas lidas e executadas, linha por linha, por algum outro programa, um intérprete. As linguagens interpretadas já foram significativamente mais lentas do que as linguagens compiladas. Mas, com o desenvolvimento da compilação just-in-time, essa lacuna está diminuindo. Exemplos de linguagens interpretadas: Python e JavaScript


### Paradigmas

Podemos dizer que paradigmas de programação são diferentes formas ou estilos em que um determinado programa ou linguagem de programação pode ser organizado. Cada paradigma consiste em certas estruturas, recursos e opiniões sobre como problemas comuns de programação devem ser abordados. Entre os principais paradigmas de programação podemos citar **programação imperativa**, **programação declarativa** e **programação orientada a objetos**. Na programação imperativa, por exemplo, o programador diz como, o quê e em qual ordem exatamente um programa ou rotina deve realizar. É neste paradigma que surgiram os famosos laços de repetição, estruturas condicionais, atribuição de valor à variáveis e controle de estado. Por outro lado, a programção declarativa não há preocupação na maneira ou método de execução de uma determinada rotina, o que importa é que a instrução seja realizada e não a forma como o será. Mais detalhes sobre paradigmas de programação podem ser obtidos [aqui](https://www.freecodecamp.org/news/an-introduction-to-programming-paradigms/#what-is-a-programming-paradigm).

Python é considerado uma linguagem de programação multi-paradigma, pois suporta orientação de objeto, programação imperativa e, em menor escala, programação funcional.


### Tipagem

O Python utiliza tipagem dinâmica e forte, isso significa que o próprio interpretador do Python infere o tipo dos dados que uma variável recebe, sem a necessidade que o usuário da linguagem diga de que tipo determinada variável é. Hoje existem inúmeras linguagens no mercado que são fortemente tipadas, referenciando especificamente o Python para explicar a questão: tipagem forte significa que o interpretador do Python avalia as expressões por conta própria e não faz coerções automáticas (conversões de valores) entre tipos de dados não compatíveis. Ao fazer operações com tipos incompatíveis, o Python não converte automaticamente esses tipos pra você, ele vai dar erro. Isso é bom, pois assim você terá a certeza que o seu resultado é mais consistente.


## Exercícios

1. X

2. X

